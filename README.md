# ðŸ“˜ RAG-Agent: Retrieval-Augmented Generation with LangGraph & LangSmith

This repository implements a complete Retrieval-Augmented Generation (RAG) pipeline using **Qdrant**, **FastEmbed sparse embeddings**, and a production-ready **LangGraph agent** featuring tools, memory, and conversation summarization.
It also includes notebooks for data preparation, retrieval experimentation, and agent prototyping.

---

## ðŸ“‚ Repository Structure

```
RAG-Agent/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chunks_fixed.json            # Finalised text chunks used for Qdrant indexing
â”‚
â”œâ”€â”€ src/agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ build_qdrant.py              # Script to build the local Qdrant DB
â”‚   â”œâ”€â”€ graph.py                     # LangGraph agent definition (RAG tool + memory)
â”‚   â””â”€â”€ requirements.txt             # Dependencies required for the agent
â”‚
â”œâ”€â”€ Chunking+Retrieval.ipynb         # Notebook: chunking, sparse embedding, retrieval experiments
â”œâ”€â”€ RAG-Agent.ipynb                  # Notebook: testing the LangGraph RAG agent
â”œâ”€â”€ Deepseek-r1.pdf                  # Source document
â”œâ”€â”€ chunks_fixed.json                # Duplicate dataset for notebook convenience
â”œâ”€â”€ ground_truth.json                # 20 labelled QA pairs for evaluation
â”œâ”€â”€ README.md                        # Project documentation
â””â”€â”€ requirements.txt                 # Notebook dependencies
```

---

# 1. Installation

### Create a clean environment (Python â‰¥ 3.11 recommended)

```bash
python3.11 -m venv rag-env
source rag-env/bin/activate
```

### Install dependencies for the agent:

```bash
pip install -r src/agent/requirements.txt
```

### Install dependencies for the notebooks:

```bash
pip install -r requirements.txt
```

---

# ðŸ“¦ 2. Build the Qdrant Database

Before running the agent, you must build the local Qdrant index:

```bash
cd src/agent
python build_qdrant.py
```

This script:

* Loads `chunks_fixed.json`
* Computes sparse BM25 embeddings (FastEmbed)
* Creates a local Qdrant database at `qdrant_db/`
* Populates the `deepseek_sparse_fixed` collection

You only need to run this **once**.

---

#  3. Running the LangGraph RAG Agent + LangSmith Integration (Debugging + Tracing)

To enable LangSmith:

```bash
export LANGSMITH_API_KEY="YOUR_KEY"
export LANGCHAIN_TRACING_V2="true"
```

Then run:

```bash
langgraph dev
```

All runs will automatically appear at:

ðŸ‘‰ **[https://smith.langchain.com](https://smith.langchain.com)**

There you can inspect:

* Graph-level execution
* Tool call results
* Message state changes
* Conversation summary updates

# 4. Notebooks (Experimentation)

### **Chunking+Retrieval.ipynb**

This notebook demonstrates:

* Data extraction from PDF
* Text chunking strategy
* Sparse BM25 embedding generation
* Retrieval evaluation using `ground_truth.json`

### **RAG-Agent.ipynb**

This notebook includes:

* Local testing for your LangGraph agent
* Tool usage examples
* Memory & summarization demonstration

# 5. Ground Truth Evaluation

The file:

```
ground_truth.json
```

Contains 20 manually curated QA pairs used during development to evaluate retrieval accuracy.

You can use this to measure precision, recall, or other IR evaluation metrics.

# ðŸ“„ Final Report â€” DeepSeek RAG Challenge

This repository contains my complete solution to the **DeepSeek RAG Challenge**.
The goal of this challenge is to demonstrate understanding of:

* Document preprocessing
* Chunking strategies
* Dense / Sparse / Hybrid retrieval
* RAG pipeline design
* Evaluation methodology
* Using an LLM to generate responses with retrieved context
* Building an advanced agent with LangGraph

# Objective

Given the **DeepSeek-R1 technical report (PDF)**, the task is to:

1. Prepare the document (cleaning, chunking, processing)
2. Implement multiple retrieval strategies
3. Evaluate retrieval performance
4. Choose the best-performing method
5. Build a full RAG system and an advanced LangGraph agent
6. Explain decisions, findings, and trade-offs

# 1. Data Preparation

## 1.1 PDF Processing

The DeepSeek-R1 technical report was extracted using **pypdf** with page-level text extraction to ensure all paragraphs, headings, and figure captions were captured.
The raw text was then normalised and cleaned for chunking experiments.

## 1.2 Chunking Experiments

To study how chunking affects retrieval performance, two major chunking strategies were evaluated:

### **A. Fixed-size Chunking**

Using LangChainâ€™s `RecursiveCharacterTextSplitter`:

```python
chunk_size = 800
chunk_overlap = 150
```
âœ” Produces consistent chunk sizes
âœ” Good for embedding-based retrieval
âœ” Works extremely well with sparse retrieval
âœ” Ensures meaningful semantic units without being too large

### **B. Heading-Based Chunking**

Chunk boundaries were derived from structural section headers using a regex:

```regex
\n(?=\d[\d\.]*\s[A-Z])
```

This matches headings like:

* `1 Introduction`
* `2.3 DeepSeek-R1-Zero`

Advantages:

âœ” Preserves logical document structure
âœ” Human-aligned semantic segmentation

# 2. Retrieval Component

Three retrieval strategies were implemented and compared.

## **A. Dense Retrieval**

Embedding models tested:

* `jinaai/jina-embeddings-v2-small-en`
* `BAAI/bge-large-en-v1.5`
* `sentence-transformers/all-MiniLM-L6-v2`

## **B. Sparse Retrieval (BM25)**

* Implemented using **FastEmbed** + **Qdrant SparseVectors**
* Stored directly in Qdrant as sparse embeddings
* No dense vectors required

## **C. Hybrid Retrieval (Dense + Sparse + ColBERT Reranking)**

Pipeline:

1. Dense vector retrieval
2. Sparse BM25 retrieval
3. ColBERT late-interaction reranker
4. Merge + rerank

Although hybrid retrieval improved certain semantic queries, **it did not outperform pure sparse retrieval**.

# 3. Evaluation

Each retrieval method was tested using **20 manually curated ground-truth Q&A pairs** derived from the DeepSeek-R1 paper.

Metrics:

* **Hit@k** (k âˆˆ {1, 3, 5, 10})
* **MRR (Mean Reciprocal Rank)**

All combinations of:

* Dense / Sparse / Hybrid retrieval
* Fixed-size / Heading-based chunking

were evaluated.

## 3.1 ðŸ“ˆ Quantitative Results
<img width="469" height="165" alt="Result" src="https://github.com/user-attachments/assets/5e5679de-1d64-49a8-bcd3-cc39ba9c2d71" />

## ðŸ† Key Takeaways

### **Sparse Retrieval Outperformed Dense and Hybrid Approaches**

Sparse BM25 achieved:

* **Best Hit@1**
* **Best Hit@5**
* **Best Hit@10**
* **Best MRR overall**


## âœ… **Final Retrieval Choice**

> **Fixed-size chunking (800/150) + Sparse Retrieval (BM25)**
> Best balance of accuracy, simplicity, cost, and robustness


# 4. Final RAG System Architecture (LangGraph Agent)

The final system integrates:

* Sparse retrieval (tool)
* LLM reasoning
* Conversation summarisation
* Persistent memory
* ReAct-style routing

## 4.1 Components
<img width="258" height="334" alt="Screenshot 2025-12-05 at 15 59 56" src="https://github.com/user-attachments/assets/87cce866-5e5f-493f-89f9-b9c1463d3966" />

### **1) Sparse Retrieval Tool**

* Uses BM25 via Qdrant
* Returns the top-k relevant chunks
* Triggered only when needed

### **2) LLM Reasoning Layer (Groq)**

Responsibilities:

* Decide whether retrieval is needed
* Consume retrieved context
* Integrate conversation summary
* Produce final responses

### **3) LangGraph Controller**

Handles:

* Node execution
* Conditional routing
* ReAct-style tool invocation loops
* Automatic summarisation

### **4) Memory + Summarization**

When `len(messages) > 6`:

* A summary of the conversation is generated
* Older messages are removed
* Summary injected into system prompt

This enables long conversations without exceeding token limits.

## 4.3 ðŸ”„ ReAct-style Interaction

1. Assistant interprets user input
2. If external information is needed â†’ triggers `rag_search` tool
3. Tool returns JSON context
4. The assistant uses the retrieved data to answer
5. Memory module summarises when conversation grows

<img width="965" height="764" alt="Screenshot 2025-12-05 at 14 48 40" src="https://github.com/user-attachments/assets/40a8eee1-0706-4ffe-b138-d6dde365c599" />


